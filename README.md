# Kaggle-Walmart-2014  
### Walmart Recruiting - Store Sales Forecasting  
Use historical markdown data to predict store sales  

## 【目的】  
アメリカのウォルマート４５店舗の売上データや店舗データ、プロモーションデータ（値下げ、販促用のマークダウンデータなど）をもとに売上予測モデルを作成する。さらに、Prominent HolidayでのMarkdownの効果も兼ねた予測モデルも作成する。  
https://www.kaggle.com/c/walmart-recruiting-store-sales-forecasting  
  
## 【流れ】
 - ①データの確認：欠損・ノイズ確認、必要な変数追加
 - ②データの準備：データ結合、欠損値確認
 - ③データ可視化・探索（EDA）：基本統計量、分布、目的変数との傾向（散布図、ヒストグラム、箱ひげ）や相関係数、モデル決定
 - ④データの前処理、売上予測モデル（回帰）を作成
 - ⑤モデルの検証：評価とパラメーター最適化

  
## 【データセット】  
目的変数：’Weekly_Sales’ 店舗・部門ごとの週売上  
説明変数：店舗No.、部門No.、締めの日時（金曜日締め）、休日ラベル、温度、燃料の料金、プロモーションデータMarkDown1-5、CPI（消費者物価指数）、失業率  
  
## 【前処理、特徴量選択】  
データの欠損値数、EDAより、
・週売上は、週・月での売上のパターンがあり、重要な特徴量としてモデルに加える。  
・不定期で変化がある特徴量★（Fuel_Price、CPI、Temperture、Unemploymentは）、週売上との相関や傾向が見えなかったため、データから削除した。  
  
特徴量を大きく分けて２パターンでモデリングを試すことにした。  
１．特徴量★、欠損値の多いMarkDown1~5を削除したデータセットを用いたモデリング  
https://github.com/YuyaIwamoto55/Kaggle-Walmart-2014/blob/main/walmart_model1_WMAE_1515_RFR.ipynb  

２．特徴量★のみを削除したモデリング  
https://github.com/YuyaIwamoto55/Kaggle-Walmart-2014/blob/main/walmart_model2_withMarkDown_WMAE_2292_RFR.ipynb  

## 【モデリング】  
結論、決定木のような数字を場合わけで振り分けてくれるモデルを採用した。  
(利用したモデル：決定木、ランダムフォレスト、XGBoost、LightGBM)  
理由として、特徴量選択より連続データを使わないことを考え、線形回帰のような相関性を重視するモデルは利用しない事とした。  

## 【検証】  
交差検証を含むGrid Searchを行い、ハイパーパラメーターの最適化を行った。　
WMAEが一番小さかったRandamForestRegressorを採用し、Google Colabのスペックの可能な限り、Grid Searchで最適なパラメーターを探す。精度に大きな影響を及ぼすn_estimators（決定木の数）を設定した。  
   
１．特徴量★、欠損値の多いMarkDown1~5を削除し、モデリング  
WMAE：1506  
２．特徴量★のみを削除したモデリング  
WMAE：2292  

特徴量の重要度は、上位から見ていくとDept、Size、Store、week、Type、Year、IsHolidayとなった。
'Week'は、11月〜12月にガツッと上昇していくような特徴的な売上推移から重要な特徴量だと考えたが重要度は低い結果となった。

## 【考察】  
　今回の結果に対する考察と改善案を以下に示した。  
* 部門'Dept'は、売上が高い店舗番号がバラバラであったため、モデルの際に複雑な予測をしてしまったと考えられる。改善としては、①One-Hot Encoding、②ランキングに並べて、再度ラベル付を行うが考えられる。
* 'Size'は、'Store'店舗のサイズを表し、部門問わず同じ番号を結合してしているので削除しても精度は変わらないので計算コストを考え削除する。
* 'Store'店舗も、'Dept'と同じ対応
* 'Week'は、このままで良いと考えます。週番号で売上の傾向はわかるので時系列データでの深層学習は必要ないと考える。
* 大量のデータを扱うことができるRAPIDSを活用し、ランダムフォレストを行う。


## 【まとめ】

　今回は、データセットにはモデリングに利用できそうな特徴量が少なかったため、  
深層学習や線形回帰は利用せず、決定木モデルを中心にモデリングを行った。  
(利用したモデル：決定木、ランダムフォレスト、XGBoost、LightGBM)  

　その結果、最適なモデルランダムフォレスト＋GridSearchを用いて、損失関数WMAE:1506の結果を出すことができた。  
しかし、モデルの検証に記した前処理で改善することでより精度は上がってくると考え、改善の余地は多いと感じています。  
個人的にGoogle ColabのスペックやM1 Mac PCの不具合でクラッシュしてしまうことが多く、計算コストを考えながらのデータ処理を意識する必要があると感じました。


ーーーーーーーー

### 【残課題】

◆前処理
・店舗、部門No.に対して、Frequency Encoding、One-hot encoding  
・Prominent Holidayにラベル付した特徴量を加える。
・売り上げが高くなる傾向のあるDeptを持っている店舗にマークをつける。
・パラメントリックモデルを採用する場合は、正規分布に寄せるBox-Cox変換や対数変換指数変換を行う。

◆データ追加  
あれば良いと思ったオープンデータの追加  
・店舗の住所がわかれば、天気データ・駅からの距離などを加えると、来店率にも影響が出るため特徴量としては良いかもしてない。
・レジ打ちの回数→平均単価

◆モデリング
Stacking：https://www.mdpi.com/2306-5729/4/1/15  
RAPIDSのcuMLのランダムフォレストを試してみる  

◆ハイパーパラメーターの選択
直行表を用いて、ある程度でも良いので複数のパラメーターの最適化をおこなう。  
https://datachemeng.com/designofexperimentscodes/  

ーーーーーーーーー
